{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d82fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# a is an array of pixels in a image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2604f9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes list with trained object name\n",
    "classes = None\n",
    "with open('coco.names', 'r') as f:\n",
    "    classes = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64c30d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['person',\n",
       " 'bicycle',\n",
       " 'car',\n",
       " 'motorbike',\n",
       " 'aeroplane',\n",
       " 'bus',\n",
       " 'train',\n",
       " 'truck',\n",
       " 'boat',\n",
       " 'traffic light',\n",
       " 'fire hydrant',\n",
       " 'stop sign',\n",
       " 'parking meter',\n",
       " 'bench',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'dog',\n",
       " 'horse',\n",
       " 'sheep',\n",
       " 'cow',\n",
       " 'elephant',\n",
       " 'bear',\n",
       " 'zebra',\n",
       " 'giraffe',\n",
       " 'backpack',\n",
       " 'umbrella',\n",
       " 'handbag',\n",
       " 'tie',\n",
       " 'suitcase',\n",
       " 'frisbee',\n",
       " 'skis',\n",
       " 'snowboard',\n",
       " 'sports ball',\n",
       " 'kite',\n",
       " 'baseball bat',\n",
       " 'baseball glove',\n",
       " 'skateboard',\n",
       " 'surfboard',\n",
       " 'tennis racket',\n",
       " 'bottle',\n",
       " 'wine glass',\n",
       " 'cup',\n",
       " 'fork',\n",
       " 'knife',\n",
       " 'spoon',\n",
       " 'bowl',\n",
       " 'banana',\n",
       " 'apple',\n",
       " 'sandwich',\n",
       " 'orange',\n",
       " 'broccoli',\n",
       " 'carrot',\n",
       " 'hot dog',\n",
       " 'pizza',\n",
       " 'donut',\n",
       " 'cake',\n",
       " 'chair',\n",
       " 'sofa',\n",
       " 'pottedplant',\n",
       " 'bed',\n",
       " 'diningtable',\n",
       " 'toilet',\n",
       " 'tvmonitor',\n",
       " 'laptop',\n",
       " 'mouse',\n",
       " 'remote',\n",
       " 'keyboard',\n",
       " 'cell phone',\n",
       " 'microwave',\n",
       " 'oven',\n",
       " 'toaster',\n",
       " 'sink',\n",
       " 'refrigerator',\n",
       " 'book',\n",
       " 'clock',\n",
       " 'vase',\n",
       " 'scissors',\n",
       " 'teddy bear',\n",
       " 'hair drier',\n",
       " 'toothbrush']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78dfb9b",
   "metadata": {},
   "source": [
    "# NOTE: net variable holds neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "860ccb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNet('yolov3.weights', 'yolov3.cfg')\n",
    "# read pretrained yolov3 model from weights and config file\n",
    "# The weights file (yolov3.weights) contains the learned parameters of the YOLOv3 model,\n",
    "# including the weights of the neural network's layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aab611c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(r\"C:\\Users\\HARISH A K\\Desktop\\python\\Deep_Learning\\Deep_learning_projects\\human_detection_in_footage\\sample2.jfif\")\n",
    "height, width = image.shape[:2] # yolov cantains 1st 2 as height and weight\n",
    "blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), swapRB=True, crop=False)\n",
    "net.setInput(blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6e056dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = net.forward(net.getUnconnectedOutLayersNames())\n",
    "# net.getUnconnectedOutLayersNames(): This method retrieves the names of the unconnected output layers of the neural network\n",
    "# net.forward(...): This function is used to perform a forward pass of the neural network. \n",
    "# A forward pass involves passing the input data through the network's layers to obtain the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a58dcd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.05365018, 0.05884587, 0.3532788 , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.04892453, 0.03397522, 0.30836058, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.04801371, 0.04328671, 0.86561227, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.954852  , 0.9450227 , 0.3873407 , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.9575348 , 0.96462333, 0.3224339 , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.96665597, 0.9607834 , 0.83714205, ..., 0.        , 0.        ,\n",
       "         0.        ]], dtype=float32),\n",
       " array([[0.02696408, 0.02945836, 0.05510319, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.02137303, 0.02427736, 0.26840758, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.0251774 , 0.01975079, 0.08691705, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.9809921 , 0.9703064 , 0.03630849, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.9783566 , 0.97499466, 0.27730736, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.97586596, 0.97812545, 0.08209734, ..., 0.        , 0.        ,\n",
       "         0.        ]], dtype=float32),\n",
       " array([[0.00710399, 0.0150694 , 0.01389438, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.00988119, 0.01400471, 0.02773201, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.01087409, 0.01273188, 0.11280921, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.991345  , 0.9904312 , 0.01692358, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.98992294, 0.9867163 , 0.01779027, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.98797643, 0.9903326 , 0.15899882, ..., 0.        , 0.        ,\n",
       "         0.        ]], dtype=float32))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs # with this output bounding boxes can be classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42a58a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center_x  315\n",
      "center_y  225\n",
      "w  70\n",
      "h  37\n",
      "x  280\n",
      "y  206\n",
      "confidence  0.0\n",
      "indices  [136 116 134 119 147  97 140  93 114 118 100  86 113 139  85  88  74  75\n",
      "  48 153  58  52  50  77  64  21 142  71  42  41  30  67  32 111  40 120\n",
      "  24  20  38  28  51  45   1  19  23  90  37  17  22  33  16  39  25  69\n",
      "  18  26]\n"
     ]
    }
   ],
   "source": [
    "conf_threshold = 0.5\n",
    "nms_threshold = 0.4\n",
    "\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        # The first 5 values in each detection represent the bounding box coordinates (x, y, w, h) and the confidence score\n",
    "        class_id = np.argmax(scores) # index of maximum value in score \n",
    "        confidence = scores[class_id]\n",
    "        if confidence > conf_threshold and class_id == 0:  # 0 corresponds to the 'person' class\n",
    "            center_x = int(detection[0] * width)\n",
    "            center_y = int(detection[1] * height)\n",
    "            w = int(detection[2] * width)\n",
    "            h = int(detection[3] * height)\n",
    "\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "\n",
    "# The first 4 values in the detection variable represent the bounding box coordinates in the normalized coordinate system. \n",
    "# The normalized coordinate system is a coordinate system where the width and height of the image are both 1.0. \n",
    "# To convert the normalized coordinates to the actual coordinates,\n",
    "# we need to multiply them by the width and height of the image\n",
    "            \n",
    "            class_ids.append(class_id)\n",
    "            confidences.append(float(confidence))\n",
    "            boxes.append([x, y, w, h])\n",
    "\n",
    "indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
    "\n",
    "# cv2.dnn.NMSBoxes() used to remove overlapping bounding boxes, leaving only the most confident bounding boxes.\n",
    "# boxes - list of bounding boxes\n",
    "# confidence - list of confidence score for each box\n",
    "# conf_threshold - minimum confidence required for the bounding box.\n",
    "# nms_threshold -  when 2 bounding boxes overlapping means higher nms_threshold value bounding box considered\n",
    "# list of indices of the bounding boxes that remain after non-maximum suppression.\n",
    "# The indices are sorted in descending order of confidence scores.\n",
    "\n",
    "print(\"center_x \",center_x)\n",
    "print(\"center_y \",center_y)\n",
    "print(\"w \",w)\n",
    "print(\"h \",h)\n",
    "print(\"x \",x)\n",
    "print(\"y \",y)\n",
    "print(\"confidence \",confidence)\n",
    "print(\"indices \",indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a2564b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for i in indices:\n",
    "    box = boxes[i]\n",
    "    x, y, w, h = box\n",
    "    label = f\"Person: {confidences[i]:.2f}\"\n",
    "    color = (0, 255, 0)  # Green color for bounding box\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "    cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    count+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5418a2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cout of people is  56\n"
     ]
    }
   ],
   "source": [
    "# Resize the image to a smaller size for displaying\n",
    "resize_factor = 1  # You can adjust this factor as needed\n",
    "small_image = cv2.resize(image, None, fx=resize_factor, fy=resize_factor)\n",
    "\n",
    "# Display the resized image with detected people\n",
    "cv2.imshow('Detected People', small_image)\n",
    "print(\"Total cout of people is \",count)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
