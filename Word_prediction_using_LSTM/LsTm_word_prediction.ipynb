{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6df14cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential # building neural networks layer by layer sequentially.\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "# dense - Connects every neuron in the previous layer to every neuron in the next layer.\n",
    "# embedding - converts integer to vectors\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#The Tokenizer class in Keras is used to vectorize text into integer sequences \n",
    "#that can be used as input to neural network models\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#his is useful when working with text data where sentences can have different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b01d9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286\n"
     ]
    }
   ],
   "source": [
    "text_data = [\n",
    "  \"The quick brown fox jumped over the lazy dog.\",\n",
    "  \"She sells seashells by the seashore.\",\n",
    "  \"Peter Piper picked a peck of pickled peppers.\",\n",
    "  \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\",\n",
    "  \"The itsy bitsy spider crawled up the water spout.\",\n",
    "  \"Humpty Dumpty sat on a wall, Humpty Dumpty had a great fall.\",\n",
    "  \"Jack and Jill went up the hill to fetch a pail of water.\",\n",
    "  \"Mary had a little lamb whose fleece was white as snow.\",\n",
    "  \"Baa baa black sheep, have you any wool?\",\n",
    "  \"Twinkle, twinkle little star, how I wonder what you are.\",\n",
    "  \"Roses are red, violets are blue, sugar is sweet, and so are you.\",\n",
    "  \"I scream, you scream, we all scream for ice cream!\", \n",
    "  \"The early bird gets the worm.\",\n",
    "  \"The grass is always greener on the other side.\",\n",
    "  \"When it rains, it pours.\",\n",
    "  \"A penny saved is a penny earned.\",\n",
    "  \"The pen is mightier than the sword.\",\n",
    "  \"Laughter is the best medicine.\",\n",
    "  \"Waste not, want not.\",\n",
    "  \"A picture is worth a thousand words.\",\n",
    "  \"Absence makes the heart grow fonder.\",\n",
    "  \"Out of sight, out of mind.\",\n",
    "  \"The grass isn't always greener on the other side.\",\n",
    "  \"Don't put all your eggs in one basket.\",  \n",
    "  \"A leopard can't change its spots.\",\n",
    "  \"Time flies when you're having fun.\",\n",
    "  \"It isn't over until the fat lady sings.\",\n",
    "  \"The best things in life are free.\",\n",
    "  \"It takes one to know one.\",\n",
    "  \"Beggars can't be choosers.\",\n",
    "  \"Fortune favors the bold.\",\n",
    "  \"The squeaky wheel gets the grease.\",\n",
    "  \"You can catch more flies with honey than with vinegar.\",\n",
    "  \"The journey of a thousand miles begins with a single step.\",\n",
    "  \"All's well that ends well.\",\n",
    "  \"It's always darkest before the dawn.\",\n",
    "  \"A picture paints a thousand words.\",\n",
    "  \"When the cat's away, the mice will play.\",\n",
    "  \"The early bird gets the worm.\",\n",
    "  \"Rome wasn't built in a day.\", \n",
    "  \"If at first you don't succeed, try, try again.\",\n",
    "  \"Fool me once, shame on you; fool me twice, shame on me.\",\n",
    "  \"Two wrongs don't make a right.\",\n",
    "  \"Easy come, easy go.\",\n",
    "  \"You can't teach an old dog new tricks.\",\n",
    "  \"Don't bite the hand that feeds you.\",\n",
    "  \"You can lead a horse to water but you can't make it drink.\",\n",
    "  \"Don't count your chickens before they hatch.\",\n",
    "  \"Necessity is the mother of invention.\",\n",
    "  \"Better late than never.\",\n",
    "  \"Look before you leap.\",\n",
    "  \"Two peas in a pod.\",\n",
    "  \"Appearances can be deceiving.\",  \n",
    "  \"Curiosity killed the cat.\",\n",
    "  \"It's not over till the fat lady sings.\",\n",
    "  \"The more things change, the more they stay the same.\",\n",
    "  \"Actions speak louder than words.\",\n",
    "  \"It never rains but pours.\",\n",
    "  \"Truth will out.\",\n",
    "  \"No man is an island.\",\n",
    "  \"The best laid plans of mice and men often go awry.\",\n",
    "  \"A drowning man will clutch at a straw.\",\n",
    "  \"An eye for an eye only ends up making the whole world blind.\",\n",
    "  \"The road to hell is paved with good intentions.\",\n",
    "  \"Where there's a will, there's a way.\",\n",
    "  \"Still waters run deep.\",\n",
    "  \"Variety is the spice of life.\",\n",
    "  \"History repeats itself.\",\n",
    "  \"The devil is in the details.\",\n",
    "  \"When life gives you lemons, make lemonade.\"\n",
    "]\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "print(total_words) # total unique number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9156341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input sequences and labels\n",
    "input_sequences = []\n",
    "for line in text_data:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "#this creates N-gram style input sequences from each text line by truncating(Shot the size)\n",
    "#the tokenized sequence up to each token's index. This can be used to train sequence prediction models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a52bd2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences for consistent input length\n",
    "max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n",
    "# makes all inuput sizes are equal by adding 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82d7fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictors and labels\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "# input_sequences[:, :-1], this takes all the time steps except the last one into the predictors tensor X.\n",
    "# input_sequences[:, -1] selects just the last time step and puts it into the labels tensor y.\n",
    "# X is a 3D tensor of shape (num_sequences, max_len - 1, num_features)\n",
    "# y is a 2D tensor of shape (num_sequences, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f32c16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 50, input_length=max_sequence_len - 1))\n",
    "# Adds an Embedding layer that maps integer word indices to 50-dimensional vectors\n",
    "# total_words is the vocabulary size\n",
    "# input_length is the max sequence length for the input sequences\n",
    "model.add(LSTM(100))\n",
    "# Adds a LSTM (Long Short Term Memory) layer with 100 memory units\n",
    "# This processes the embedded sequence input\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "# Adds a dense output layer with a unit for each word in the vocabulary\n",
    "# Uses softmax to output a probability distribution over all possible words\n",
    "# softmax([2.1, 0.8, 1.5]) = [0.462, 0.172, 0.366]  (HIGHER SCORE GETS HIGHER PROBABILITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "741d825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#Configures the model for training\n",
    "#Uses sparse categorical crossentropy loss for classification\n",
    "#Uses adam optimizer for training\n",
    "#Metrics to track are accuracy\n",
    "#This model takes integer encoded sequences as input, passes them through an embedding + LSTM to extract features, \n",
    "#and predicts the next word with a softmax output layer.\n",
    "\n",
    "#The model can then be trained to maximize prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "906130ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14/14 [==============================] - 24s 70ms/step - loss: 5.6526 - accuracy: 0.0319\n",
      "Epoch 2/100\n",
      "14/14 [==============================] - 1s 63ms/step - loss: 5.5956 - accuracy: 0.0592\n",
      "Epoch 3/100\n",
      "14/14 [==============================] - 1s 64ms/step - loss: 5.3820 - accuracy: 0.0592\n",
      "Epoch 4/100\n",
      "14/14 [==============================] - 1s 55ms/step - loss: 5.2743 - accuracy: 0.0592\n",
      "Epoch 5/100\n",
      "14/14 [==============================] - 1s 46ms/step - loss: 5.2392 - accuracy: 0.0592\n",
      "Epoch 6/100\n",
      "14/14 [==============================] - 1s 43ms/step - loss: 5.2187 - accuracy: 0.0592\n",
      "Epoch 7/100\n",
      "14/14 [==============================] - 1s 48ms/step - loss: 5.1866 - accuracy: 0.0615\n",
      "Epoch 8/100\n",
      "14/14 [==============================] - 1s 36ms/step - loss: 5.1460 - accuracy: 0.0615\n",
      "Epoch 9/100\n",
      "14/14 [==============================] - 1s 36ms/step - loss: 5.1050 - accuracy: 0.0615\n",
      "Epoch 10/100\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 5.0375 - accuracy: 0.0683\n",
      "Epoch 11/100\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 4.9838 - accuracy: 0.0592\n",
      "Epoch 12/100\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 4.9128 - accuracy: 0.0706\n",
      "Epoch 13/100\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 4.8481 - accuracy: 0.0752\n",
      "Epoch 14/100\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 4.7769 - accuracy: 0.0706\n",
      "Epoch 15/100\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 4.7283 - accuracy: 0.0774\n",
      "Epoch 16/100\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 4.6745 - accuracy: 0.0820\n",
      "Epoch 17/100\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 4.6101 - accuracy: 0.0911\n",
      "Epoch 18/100\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 4.5504 - accuracy: 0.1002\n",
      "Epoch 19/100\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 4.5137 - accuracy: 0.0979\n",
      "Epoch 20/100\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 4.4363 - accuracy: 0.1025\n",
      "Epoch 21/100\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 4.3741 - accuracy: 0.1162\n",
      "Epoch 22/100\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 4.3118 - accuracy: 0.1207\n",
      "Epoch 23/100\n",
      "14/14 [==============================] - 1s 36ms/step - loss: 4.2405 - accuracy: 0.1276\n",
      "Epoch 24/100\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 4.1635 - accuracy: 0.1435\n",
      "Epoch 25/100\n",
      "14/14 [==============================] - 1s 39ms/step - loss: 4.0896 - accuracy: 0.1412\n",
      "Epoch 26/100\n",
      "14/14 [==============================] - 1s 48ms/step - loss: 4.0246 - accuracy: 0.1526\n",
      "Epoch 27/100\n",
      "14/14 [==============================] - 1s 51ms/step - loss: 3.9407 - accuracy: 0.1663\n",
      "Epoch 28/100\n",
      "14/14 [==============================] - 1s 56ms/step - loss: 3.8718 - accuracy: 0.1754\n",
      "Epoch 29/100\n",
      "14/14 [==============================] - 1s 80ms/step - loss: 3.7930 - accuracy: 0.1959\n",
      "Epoch 30/100\n",
      "14/14 [==============================] - 1s 93ms/step - loss: 3.7256 - accuracy: 0.2027\n",
      "Epoch 31/100\n",
      "14/14 [==============================] - 1s 81ms/step - loss: 3.6538 - accuracy: 0.2210\n",
      "Epoch 32/100\n",
      "14/14 [==============================] - 1s 71ms/step - loss: 3.5556 - accuracy: 0.2369\n",
      "Epoch 33/100\n",
      "14/14 [==============================] - 1s 75ms/step - loss: 3.4812 - accuracy: 0.2551\n",
      "Epoch 34/100\n",
      "14/14 [==============================] - 1s 107ms/step - loss: 3.4002 - accuracy: 0.2665\n",
      "Epoch 35/100\n",
      "14/14 [==============================] - 1s 95ms/step - loss: 3.3203 - accuracy: 0.2870\n",
      "Epoch 36/100\n",
      "14/14 [==============================] - 1s 99ms/step - loss: 3.2422 - accuracy: 0.3007\n",
      "Epoch 37/100\n",
      "14/14 [==============================] - 1s 81ms/step - loss: 3.1596 - accuracy: 0.3075\n",
      "Epoch 38/100\n",
      "14/14 [==============================] - 1s 77ms/step - loss: 3.0849 - accuracy: 0.3257\n",
      "Epoch 39/100\n",
      "14/14 [==============================] - 1s 87ms/step - loss: 3.0215 - accuracy: 0.3462\n",
      "Epoch 40/100\n",
      "14/14 [==============================] - 1s 85ms/step - loss: 2.9444 - accuracy: 0.3599\n",
      "Epoch 41/100\n",
      "14/14 [==============================] - 1s 104ms/step - loss: 2.8620 - accuracy: 0.3804\n",
      "Epoch 42/100\n",
      "14/14 [==============================] - 1s 89ms/step - loss: 2.8079 - accuracy: 0.3941\n",
      "Epoch 43/100\n",
      "14/14 [==============================] - 1s 91ms/step - loss: 2.7280 - accuracy: 0.4169\n",
      "Epoch 44/100\n",
      "14/14 [==============================] - 1s 94ms/step - loss: 2.6461 - accuracy: 0.4487\n",
      "Epoch 45/100\n",
      "14/14 [==============================] - 1s 100ms/step - loss: 2.5674 - accuracy: 0.4670\n",
      "Epoch 46/100\n",
      "14/14 [==============================] - 1s 89ms/step - loss: 2.4931 - accuracy: 0.4692\n",
      "Epoch 47/100\n",
      "14/14 [==============================] - 1s 84ms/step - loss: 2.4186 - accuracy: 0.4966\n",
      "Epoch 48/100\n",
      "14/14 [==============================] - 1s 77ms/step - loss: 2.3616 - accuracy: 0.5216\n",
      "Epoch 49/100\n",
      "14/14 [==============================] - 1s 73ms/step - loss: 2.2908 - accuracy: 0.5467\n",
      "Epoch 50/100\n",
      "14/14 [==============================] - 1s 74ms/step - loss: 2.2252 - accuracy: 0.5740\n",
      "Epoch 51/100\n",
      "14/14 [==============================] - 1s 75ms/step - loss: 2.1662 - accuracy: 0.5831\n",
      "Epoch 52/100\n",
      "14/14 [==============================] - 1s 94ms/step - loss: 2.0938 - accuracy: 0.6036\n",
      "Epoch 53/100\n",
      "14/14 [==============================] - 1s 71ms/step - loss: 2.0402 - accuracy: 0.6219\n",
      "Epoch 54/100\n",
      "14/14 [==============================] - 1s 77ms/step - loss: 1.9803 - accuracy: 0.6629\n",
      "Epoch 55/100\n",
      "14/14 [==============================] - 1s 85ms/step - loss: 1.9192 - accuracy: 0.6765\n",
      "Epoch 56/100\n",
      "14/14 [==============================] - 1s 87ms/step - loss: 1.8604 - accuracy: 0.6902\n",
      "Epoch 57/100\n",
      "14/14 [==============================] - 1s 81ms/step - loss: 1.8054 - accuracy: 0.7084\n",
      "Epoch 58/100\n",
      "14/14 [==============================] - 1s 73ms/step - loss: 1.7563 - accuracy: 0.7312\n",
      "Epoch 59/100\n",
      "14/14 [==============================] - 1s 70ms/step - loss: 1.7047 - accuracy: 0.7289\n",
      "Epoch 60/100\n",
      "14/14 [==============================] - 1s 78ms/step - loss: 1.6549 - accuracy: 0.7426\n",
      "Epoch 61/100\n",
      "14/14 [==============================] - 1s 78ms/step - loss: 1.6038 - accuracy: 0.7517\n",
      "Epoch 62/100\n",
      "14/14 [==============================] - 1s 78ms/step - loss: 1.5549 - accuracy: 0.7699\n",
      "Epoch 63/100\n",
      "14/14 [==============================] - 1s 76ms/step - loss: 1.5010 - accuracy: 0.7699\n",
      "Epoch 64/100\n",
      "14/14 [==============================] - 1s 83ms/step - loss: 1.4588 - accuracy: 0.7882\n",
      "Epoch 65/100\n",
      "14/14 [==============================] - 1s 81ms/step - loss: 1.4093 - accuracy: 0.7973\n",
      "Epoch 66/100\n",
      "14/14 [==============================] - 1s 81ms/step - loss: 1.3665 - accuracy: 0.8041\n",
      "Epoch 67/100\n",
      "14/14 [==============================] - 1s 81ms/step - loss: 1.3250 - accuracy: 0.8200\n",
      "Epoch 68/100\n",
      "14/14 [==============================] - 1s 81ms/step - loss: 1.2805 - accuracy: 0.8246\n",
      "Epoch 69/100\n",
      "14/14 [==============================] - 1s 83ms/step - loss: 1.2410 - accuracy: 0.8292\n",
      "Epoch 70/100\n",
      "14/14 [==============================] - 1s 78ms/step - loss: 1.2049 - accuracy: 0.8314\n",
      "Epoch 71/100\n",
      "14/14 [==============================] - 1s 89ms/step - loss: 1.1698 - accuracy: 0.8337\n",
      "Epoch 72/100\n",
      "14/14 [==============================] - 1s 86ms/step - loss: 1.1306 - accuracy: 0.8474\n",
      "Epoch 73/100\n",
      "14/14 [==============================] - 1s 80ms/step - loss: 1.0963 - accuracy: 0.8656\n",
      "Epoch 74/100\n",
      "14/14 [==============================] - 1s 73ms/step - loss: 1.0649 - accuracy: 0.8724\n",
      "Epoch 75/100\n",
      "14/14 [==============================] - 1s 72ms/step - loss: 1.0299 - accuracy: 0.8861\n",
      "Epoch 76/100\n",
      "14/14 [==============================] - 1s 73ms/step - loss: 1.0070 - accuracy: 0.8770\n",
      "Epoch 77/100\n",
      "14/14 [==============================] - 1s 60ms/step - loss: 0.9749 - accuracy: 0.8884\n",
      "Epoch 78/100\n",
      "14/14 [==============================] - 1s 56ms/step - loss: 0.9510 - accuracy: 0.8884\n",
      "Epoch 79/100\n",
      "14/14 [==============================] - 1s 55ms/step - loss: 0.9138 - accuracy: 0.8929\n",
      "Epoch 80/100\n",
      "14/14 [==============================] - 1s 48ms/step - loss: 0.8851 - accuracy: 0.8952\n",
      "Epoch 81/100\n",
      "14/14 [==============================] - 1s 49ms/step - loss: 0.8570 - accuracy: 0.8929\n",
      "Epoch 82/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 1s 50ms/step - loss: 0.8365 - accuracy: 0.9066\n",
      "Epoch 83/100\n",
      "14/14 [==============================] - 1s 42ms/step - loss: 0.8084 - accuracy: 0.9066\n",
      "Epoch 84/100\n",
      "14/14 [==============================] - 1s 48ms/step - loss: 0.7874 - accuracy: 0.9043\n",
      "Epoch 85/100\n",
      "14/14 [==============================] - 1s 41ms/step - loss: 0.7676 - accuracy: 0.9134\n",
      "Epoch 86/100\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.7464 - accuracy: 0.9089\n",
      "Epoch 87/100\n",
      "14/14 [==============================] - 1s 37ms/step - loss: 0.7241 - accuracy: 0.9157\n",
      "Epoch 88/100\n",
      "14/14 [==============================] - 1s 37ms/step - loss: 0.7019 - accuracy: 0.9180\n",
      "Epoch 89/100\n",
      "14/14 [==============================] - 1s 37ms/step - loss: 0.6825 - accuracy: 0.9226\n",
      "Epoch 90/100\n",
      "14/14 [==============================] - 1s 35ms/step - loss: 0.6650 - accuracy: 0.9203\n",
      "Epoch 91/100\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.6471 - accuracy: 0.9226\n",
      "Epoch 92/100\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 0.6313 - accuracy: 0.9294\n",
      "Epoch 93/100\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.6116 - accuracy: 0.9248\n",
      "Epoch 94/100\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.5967 - accuracy: 0.9294\n",
      "Epoch 95/100\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.5853 - accuracy: 0.9226\n",
      "Epoch 96/100\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.5697 - accuracy: 0.9271\n",
      "Epoch 97/100\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5540 - accuracy: 0.9294\n",
      "Epoch 98/100\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5399 - accuracy: 0.9226\n",
      "Epoch 99/100\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5259 - accuracy: 0.9248\n",
      "Epoch 100/100\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5143 - accuracy: 0.9248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x24ac1c78e50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, y, epochs=100, verbose=1)\n",
    "# epochs - Number of times to iterate over the entire input data. \n",
    "# One epoch = one forward pass + one backward pass (for gradient calculation) over all the data. \n",
    "# More epochs means more training. Common ranges are 10-100+ epochs.\n",
    "# verbose - Controls log output frequency. 0 = silent, 1 = progress bar, 2 = one line per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a920133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the next word\n",
    "def generate_next_word(seed_text):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    # Tokenize the seed text to get a sequence of integer tokens\n",
    "    \n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "    # Pad the token sequence to max length expected by the model\n",
    "\n",
    "    predicted_probs = model.predict(token_list, verbose=0)\n",
    "    # Use the trained model to predict probabilities for next token\n",
    "\n",
    "    predicted_index = np.argmax(predicted_probs)\n",
    "    # Get the index of the token with maximum predicted probability\n",
    "\n",
    "    output_word = \"\"\n",
    "    \n",
    "    for word, index in tokenizer.word_index.items(): #Loop through the vocabulary and find the word that matches the \n",
    "                                                     #predicted index\n",
    "        if index == predicted_index:\n",
    "            output_word = word\n",
    "            break\n",
    "    return output_word\n",
    "# iteratively generate a sequence word by word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea3fb2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: its better if you stay\n",
      "Generated sentence: its better if you stay the\n"
     ]
    }
   ],
   "source": [
    "# Take user input and generate next words\n",
    "while True:\n",
    "    user_input = input(\"Enter a sentence: \")\n",
    "    next_word = generate_next_word(user_input)\n",
    "    generated_text = user_input + \" \" + next_word\n",
    "    print(\"Generated sentence:\", generated_text)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a199b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Samples\n",
    "# vector will look like\n",
    "# dog -> [0.2, 0.6]  \n",
    "# cat -> [0.3, 0.4]\n",
    "# car -> [0.7, 0.2]\n",
    "# bike -> [0.1, 0.8]\n",
    "\n",
    "# tokenizer example\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "text_data = [\"The cat is cute\", \"The dog is hairy\", \"The bird can sing\"]\n",
    "\n",
    "tokenizer = Tokenizer() \n",
    "\n",
    "# Fit the tokenizer on the text data\n",
    "tokenizer.fit_on_texts(text_data) \n",
    "\n",
    "# Convert text to integer sequences\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "print(sequences)\n",
    "print(tokenizer.word_index)\n",
    "\n",
    "# {'the': 1, 'cat': 2, 'is': 3, 'cute': 4, 'dog': 5, 'hairy': 6, 'bird': 7, 'can': 8, 'sing': 9}\n",
    "\n",
    "# example for pad sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sequences = [[1,2,3], [1,2], [1]]\n",
    "\n",
    "padded = pad_sequences(sequences,maxlen=2)\n",
    "print(padded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
